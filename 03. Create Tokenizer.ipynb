{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a02ccc8-b7c0-4abf-93a5-c5233b76a4ea",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eafbdfd-31b9-494c-8e62-94f1c8eac09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32411ad5-0bb3-4751-bd2d-816a9fedd4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_data_dir = \"datasets/hdfs/output/\"\n",
    "hdfs_file = \"hdfs_labeled.csv\"\n",
    "hdfs_content_file = \"hdfs_content_labeled.csv\"\n",
    "hdfs_output_dir = \"tokenizer/hdfs\"\n",
    "\n",
    "bgl_data_dir = \"datasets/bgl/output/\"\n",
    "bgl_file = \"bgl_time_windowed.csv\"\n",
    "bgl_content_file = \"bgl_time_windowed_content.csv\"\n",
    "bgl_output_dir = \"tokenizer/bgl\"\n",
    "\n",
    "tbird_data_dir = \"datasets/tbird/output/\"\n",
    "tbird_file = \"tbird_time_windowed_5M.csv\"\n",
    "tbird_content_file = \"tbird_time_windowed_5M_content.csv\"\n",
    "tbird_output_dir = \"tokenizer/tbird\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d4d5de-b4e0-412e-a833-ab64906f13d5",
   "metadata": {},
   "source": [
    "## Generate Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322f3b4f-737a-4b86-a61e-6f663805e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenizer(log_dir, log_file, output_dir, token, mode=None, content=False, vocab_size = 50265):\n",
    "    start = timer()\n",
    "    df = pd.read_csv(log_dir + log_file)\n",
    "    if mode == 'hdfs':\n",
    "        if content:\n",
    "            df = df[[\"ContentSequence\"]]\n",
    "            df.rename(columns={'ContentSequence': 'text'}, inplace=True)\n",
    "        else :\n",
    "            df = df[[\"EventSequence\"]]\n",
    "            df.rename(columns={'EventSequence': 'text'}, inplace=True)\n",
    "    else :\n",
    "        if content:\n",
    "            df = df[[\"EventTemplate\"]]\n",
    "            df.rename(columns={'EventTemplate': 'text'}, inplace=True)\n",
    "        else :\n",
    "            df = df[[\"EventId\"]]\n",
    "            df.rename(columns={'EventId': 'text'}, inplace=True)\n",
    "    \n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    batch_size = 1000\n",
    "    \n",
    "    def batch_iterator():\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            yield dataset[i : i + batch_size][\"text\"]\n",
    "\n",
    "    tokenizer = token.train_new_from_iterator(batch_iterator(), vocab_size)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(\"tokenizer saved at : \" + output_dir)\n",
    "    end = timer()\n",
    "    print(\"time elapsed : {:.2f}s\".format(end-start)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e95c8ab-0a01-4e16-8c39-69e18905c88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\src\\anaconda3\\envs\\skripsi\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f8234-0eb8-41a8-9e71-06be2776c808",
   "metadata": {},
   "source": [
    "### HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b0b5931-93e0-4240-ac38-5c3ab6230def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer saved at : tokenizer/hdfs\n",
      "time elapsed : 45.70s\n"
     ]
    }
   ],
   "source": [
    "generate_tokenizer(hdfs_data_dir, hdfs_file, hdfs_output_dir, tokenizer, mode='hdfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a771a48b-f108-4254-a00f-039541cc84ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer saved at : tokenizer/hdfs/content\n",
      "time elapsed : 110.74s\n"
     ]
    }
   ],
   "source": [
    "generate_tokenizer(hdfs_data_dir, hdfs_content_file, hdfs_output_dir + \"/content\", tokenizer, mode='hdfs', content=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3006c37d-be85-4447-8ade-d69bc1d3fad5",
   "metadata": {},
   "source": [
    "## BGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b64f88c1-8d61-403b-8d42-1275e1e7565f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer saved at : tokenizer/bgl\n",
      "time elapsed : 10.88s\n"
     ]
    }
   ],
   "source": [
    "generate_tokenizer(bgl_data_dir, bgl_file, bgl_output_dir, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f76525f4-c89d-45ea-afbc-6da903f1361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer saved at : tokenizer/bgl/content\n",
      "time elapsed : 32.81s\n"
     ]
    }
   ],
   "source": [
    "generate_tokenizer(bgl_data_dir, bgl_content_file, bgl_output_dir + \"/content\", tokenizer, content=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0154ce-33e6-4025-9431-698b66643161",
   "metadata": {},
   "source": [
    "### Thunderbird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1657cab-3c20-4dec-967b-37766919a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer saved at : tokenizer/tbird\n",
      "time elapsed : 25.12s\n"
     ]
    }
   ],
   "source": [
    "generate_tokenizer(tbird_data_dir, tbird_file, tbird_output_dir, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c6ec493-8965-4c55-a1fb-3d911f88c47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer saved at : tokenizer/tbird/content\n",
      "time elapsed : 139.66s\n"
     ]
    }
   ],
   "source": [
    "generate_tokenizer(tbird_data_dir, tbird_content_file, tbird_output_dir + \"/content\", tokenizer, content=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df44043-564c-4553-8d34-636911bed8d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f50144a-a888-4880-a46b-e36d06018795",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/hdfs/output/hdfs_labeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe0c32d-cb63-48ec-8841-7b13630ace8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"BlockId\", axis='columns')\n",
    "df = df.drop(\"Label\", axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa88170d-c4f7-488c-898b-42d5e8318523",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"EventSequence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3455071-64e3-4b47-91e3-f50904c39d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'EventSequence': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "373ba8b8-c1c7-4a82-9cbd-71b7e0b9ce71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['9b7aa7a3', '81358cb3', '9b7aa7a3', '9b7aa7a3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['9b7aa7a3', '9b7aa7a3', '81358cb3', '9b7aa7a3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['9b7aa7a3', '81358cb3', '9b7aa7a3', '9b7aa7a3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['9b7aa7a3', '81358cb3', '9b7aa7a3', '9b7aa7a3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['9b7aa7a3', '9b7aa7a3', '81358cb3', '9b7aa7a3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  ['9b7aa7a3', '81358cb3', '9b7aa7a3', '9b7aa7a3...\n",
       "1  ['9b7aa7a3', '9b7aa7a3', '81358cb3', '9b7aa7a3...\n",
       "2  ['9b7aa7a3', '81358cb3', '9b7aa7a3', '9b7aa7a3...\n",
       "3  ['9b7aa7a3', '81358cb3', '9b7aa7a3', '9b7aa7a3...\n",
       "4  ['9b7aa7a3', '9b7aa7a3', '81358cb3', '9b7aa7a3..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "288d6784-7bd9-42e6-9184-9cb6b47b404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016aec46-5f6c-4894-8357-d291b215a15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 575061\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5d0a15a-7c84-473c-ad9c-1989943745b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "all_texts = [dataset[i : i + batch_size][\"text\"] for i in range(0, len(dataset), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40f53461-1eb3-443d-bdd8-e1790a9f74a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator():\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5a9c3f4-83db-49f3-aaeb-fde4a58c32bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\src\\anaconda3\\envs\\skripsi\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "255883e6-24c5-4b9d-a765-ee15873e6f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6af412e-f75f-45bd-911f-9478da17c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(),vocab_size = 50265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2090148-4687-4c62-99fe-bc1661469c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 284, 29, 70, 27, 263, 27, 69, 23, 261, 262, 29, 70, 27, 263, 27, 69, 23, 261, 262, 286, 285, 23, 261, 262, 29, 70, 27, 263, 27, 69, 23, 261, 262, 22, 73, 21, 268, 20, 263, 261, 262, 271, 70, 29, 71, 269, 261, 262, 22, 73, 21, 268, 20, 263, 261, 262, 271, 70, 29, 71, 269, 261, 262, 22, 73, 21, 268, 20, 263, 261, 262, 271, 70, 29, 71, 269, 261, 262, 22, 74, 267, 71, 266, 261, 262, 22, 74, 267, 71, 266, 261, 262, 22, 74, 267, 71, 266, 261, 262, 26, 289, 25, 287, 261, 262, 72, 295, 71, 26, 261, 262, 72, 295, 71, 26, 261, 262, 264, 280, 279, 261, 262, 264, 280, 279, 261, 262, 264, 280, 279, 261, 262, 70, 277, 261, 262, 70, 277, 261, 262, 70, 277, 281, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer(dataset[1][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6b5fc29-1bdb-4304-ac36-96f6a4d1afc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/hdfs\\\\tokenizer_config.json',\n",
       " 'tokenizer/hdfs\\\\special_tokens_map.json',\n",
       " 'tokenizer/hdfs\\\\vocab.json',\n",
       " 'tokenizer/hdfs\\\\merges.txt',\n",
       " 'tokenizer/hdfs\\\\added_tokens.json',\n",
       " 'tokenizer/hdfs\\\\tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.save_pretrained(\"tokenizer/hdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9fe11c-7412-4a19-87f6-79acdbe564e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
